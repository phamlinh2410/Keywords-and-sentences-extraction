{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bearing-fabric",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vncorenlp import VnCoreNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "norwegian-fetish",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "blank-gazette",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "medium-specialist",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preTreatment(sentences, length):\n",
    "    punctuations = []     # Dấu câu\n",
    "    with open(\"punctuation.txt\", \"r\", encoding = \"utf-8\") as data:\n",
    "        for line in data:\n",
    "            punctuations.append(line.strip())\n",
    "    data.close()\n",
    "    \n",
    "    stopWords = []\n",
    "    with open(\"vietnamese-stopwords-dash.txt\", \"r\", encoding = \"utf-8\") as data:\n",
    "        for line in data:\n",
    "            stopWords.append(line.strip())\n",
    "    data.close()\n",
    "    \n",
    "    for i in range(0, length):\n",
    "        for j in range(0, len(sentences[i])):\n",
    "            sentences[i][j] = sentences[i][j].lower()\n",
    "        #print(sentences[i])\n",
    "        \n",
    "    # Loại bỏ dấu câu và stop words\n",
    "    for i in range(0, length):\n",
    "        sentences[i] = list(set(sentences[i]) - set(punctuations) - set(stopWords))\n",
    "        #print(sentences[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ranging-prevention",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjacentMatrix(sentences, length):\n",
    "    # Tạo ma trận kề của đồ thị có trọng số\n",
    "    matrix = np.zeros((length, length))\n",
    "    for i in range(0, length):\n",
    "        for j in range(0, length):\n",
    "            if i != j:\n",
    "                jaccard = len(set(sentences[i]) & set(sentences[j])) / len(set(sentences[i]) | set(sentences[j]))\n",
    "                if jaccard != 0:\n",
    "                    matrix[i][j] = jaccard\n",
    "    #for i in range(0, length):\n",
    "        #for j in range(0, length):\n",
    "            #print(matrix[i][j], end=\"\\t\")\n",
    "        #print()\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "revolutionary-threshold",
   "metadata": {},
   "outputs": [],
   "source": [
    "def textRank(matrix, length):\n",
    "    # sumOut là vector trong đó sumOut[i] là tổng trọng số của các link out của matrix[i]\n",
    "    sumOut = np.zeros(length)\n",
    "    for i in range(0, length):\n",
    "        for j in range(0, length):\n",
    "            if matrix[i][j] != 0:\n",
    "                sumOut[i] += matrix[i][j]\n",
    "    #print(sumOut)\n",
    "    \n",
    "    WS = np.ones(length)        # vector điểm của các đỉnh ở lần tính thứ n\n",
    "    preWS = np.ones(length)     # vector điểm của các đỉnh ở lần tính thứ n - 1\n",
    "    \n",
    "    d = 0.85\n",
    "    x = 0\n",
    "    \n",
    "    # Áp dụng thuật toán ranking trên đồ thị có trọng số\n",
    "    while x < 100:\n",
    "        for i in range(0, length):\n",
    "            sum = 0\n",
    "            for j in range(0, length):\n",
    "                if matrix[i][j] != 0:\n",
    "                    sum += (matrix[j][i] / sumOut[j]) * preWS[j]\n",
    "            WS[i] = (1 - d) + d * sum\n",
    "        for i in range(0, length):\n",
    "            preWS[i] = WS[i]\n",
    "        #print(\"WS lần thứ:\", x)\n",
    "        #print(WS)\n",
    "        x += 1\n",
    "    return WS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "academic-cleaners",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentencesExtraction(WS, word_segmented_text, length):\n",
    "    # Tìm đỉnh có điểm cao nhất\n",
    "    dictWS = {}\n",
    "    for i in range(0, length):\n",
    "        dictWS[WS[i]] = i\n",
    "        \n",
    "    mainSentences = []\n",
    "    \n",
    "    for i in sorted(dictWS.keys()):\n",
    "        mainSentences.append(word_segmented_text[dictWS[i]])\n",
    "    return mainSentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "future-platinum",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTF(wordDict, sentence):\n",
    "    tfDict = {}\n",
    "    wordsCount = len(sentence)\n",
    "    for sentence, count in wordDict.items():\n",
    "        tfDict[sentence] = count/float(wordsCount)\n",
    "    return tfDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "korean-vanilla",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeIDF(sentences):\n",
    "    import math\n",
    "    idfDict = {}\n",
    "    N = len(sentences)\n",
    "    \n",
    "    idfDict = dict.fromkeys(sentences[0].keys(), 0)\n",
    "    for s in sentences:\n",
    "        for word, val in s.items():\n",
    "            if val > 0:\n",
    "                idfDict[word] += 1\n",
    "    \n",
    "    for word, val in idfDict.items():\n",
    "        idfDict[word] = math.log10(N / float(val))\n",
    "        \n",
    "    return idfDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "unknown-appliance",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTFIDF(tfSentence, idfs):\n",
    "    tfidf = {}\n",
    "    for word, val in tfSentence.items():\n",
    "        tfidf[word] = val*idfs[word]\n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "median-chorus",
   "metadata": {},
   "outputs": [],
   "source": [
    "def keywordsExtractionByTFIDF(sentences):\n",
    "    wordSet = set(sentences[0])\n",
    "    for i in range(1, len(sentences)):\n",
    "        wordSet = wordSet.union(set(sentences[i]))\n",
    "    #print(wordSet)\n",
    "    wordDict = []\n",
    "    for i in range(0, len(sentences)):\n",
    "        wordDict.append(dict.fromkeys(wordSet, 0))\n",
    "    #print(wordDict)\n",
    "    for i in range(0, len(sentences)):\n",
    "        for w in sentences[i]:\n",
    "            wordDict[i][w] += 1\n",
    "    tfSentences = []\n",
    "    for i in range(0, len(sentences)):\n",
    "        tfSentences.append(computeTF(wordDict[i], sentences[i]))\n",
    "    #print(tfSentences)\n",
    "    idfs = computeIDF(wordDict)\n",
    "    #print(idfs)\n",
    "    tfidfSentences = []\n",
    "    for i in range(0, len(sentences)):\n",
    "        tfidfSentences.append(computeTFIDF(tfSentences[i], idfs))\n",
    "    #print(tfidfSentences)\n",
    "    \n",
    "    df = pd.DataFrame(tfidfSentences)\n",
    "    arr = {}\n",
    "    wordSet = list(wordSet)\n",
    "    n = (len(wordSet)*5)//100 +1\n",
    "    for i in range(0, len(wordSet)):\n",
    "        arr[wordSet[i]] = max(df[wordSet[i]])\n",
    "    keyWords = dict(sorted(arr.items(), key=lambda item: item[1], reverse=True))\n",
    "    print(\"Keywords:\")\n",
    "    for i in range(0, n):\n",
    "        print(list(keyWords.keys())[i])\n",
    "    #print(keyWords)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "mobile-bolivia",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    annotator = VnCoreNLP(r\"C:\\Users\\admin\\Desktop\\kit\\VnCoreNLP\\VnCoreNLP-1.1.1.jar\", annotators=\"wseg,pos,parse\", max_heap_size='-Xmx2g')\n",
    "    \n",
    "    data = open(\"text.txt\", \"r\", encoding=\"utf-8\")\n",
    "    text = data.read()\n",
    "    \n",
    "    annotated_text = annotator.annotate(text)\n",
    "    word_segmented_text = annotator.tokenize(text)\n",
    "    \n",
    "    sentences = word_segmented_text.copy()\n",
    "    length = len(word_segmented_text)\n",
    "    \n",
    "    preTreatment(sentences, length)\n",
    "    keywordsExtractionByTFIDF(sentences)\n",
    "    matrix = adjacentMatrix(sentences, length)\n",
    "    WS = textRank(matrix, length)\n",
    "    mainSentences = sentencesExtraction(WS, word_segmented_text, length)\n",
    "    s = \" \"\n",
    "    print(s.join(mainSentences[len(mainSentences) - 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "stylish-paraguay",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keywords:\n",
      "đối_đáp\n",
      "đại_diện\n",
      "17/4\n",
      "sửa_chữa\n",
      "trở_lại\n",
      "xã_hội\n",
      "gia_đình\n",
      "phiên_toà\n",
      "nhận_thức\n",
      "luật_sư lê_trung_sơn phân_tích thêm , khi cho ý_kiến đồng_ý với nội_dung tờ_trình số 835 , bị_cáo hưng chỉ có mong_muốn đổi_mới , tháo_gỡ khó_khăn cho dự_án .\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "later-russia",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
